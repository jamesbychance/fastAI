# Questions - Lecture 2 - Production

Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.
> missmatched sized images, trained on images that don't fully represent all possible images of a real life bear

Where do text models currently have a major deficiency?
> Text models can currently very convincingly respond to social media posts and chat in the common vernacular, in some cases more convincingly better than actual human generated responses. However, their major deficiency is that they are limited on how well they can be trainied to provide text responses within specified knowledge domains, for example, training a model on all medical texts does not lead to a model that can provide accurate medical advice. In fact, the medical advice it provides could use dangerously convincing jargon to the layman, but the advice itself would be medically dangerous (cause harm). 

What are possible negative societal implications of text generation models?
> In addition to the harm caused by medical advice generated by a text model, another negative societal implication would be troll farms on social media creating disinformation at such scales and volume that they are able to shift discourse and lead to reshaping of societal values and debates. Real examples include, widening the divide in policital discourse, vaccine hestitency and the propogation of consipracy theories that cause harm.

In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?
> In situations where a model might make mistakes that could be harmful, a good alternative for automating the process to arrive at the optimal outcome is to have a blended approach, combining a machine and human-in-the-loop process. For example, in detecting medical conditions using x-ray images for urgent medical intervention, an ML application could do an intial scan of all images prioritising certain cases for immediate examination. This ML application on average would detect most of the urgent cases, but may miss one or two. However the original human driven assessment process would still run as a safety-net to catch these misses. Therby the combined ML and human assement process of x-ray images is overall more efficient than either the machine (only) or human (only) processes alone.

What kind of tabular data is deep learning particularly good at?
> Deep learning excels at finding hidden patterns in data. For high-cardinality categorical variables, the model can automatically learn interactions between the variables without the need for extensive manual preprocessing or feature engineering. Tabluar data categories with so many unique items that theyâ€™re hard to manage in data analysis or machine learning.

What's a key downside of directly using a deep learning model for recommendation systems?
> They are good at recommending what another user has bought in the past, but not necessarily good at recommending what the current user actually needs or what would be helpful.

What are the steps of the Drivetrain Approach?
> 1. Defined objective: What outcome am I trying to achieve? 
> 2. Levers: What inputs can we control?
> 3. Data: What data can we collect?
> 4. Model: How the levers influence the objective? 

How do the steps of the Drivetrain Approach map to a recommendation system?
> The objective of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. 
> The lever is the ranking of the recommendations. 
> New data must be collected to generate recommendations that will cause new sales. 
> This will require conducting many randomised experiments in order to collect data about a wide range of recommendations for a wide range of customers. 
> This is a step that few organisations take; but without it, you don't have the information you need to actually optimise recommendations based on your true objective (more sales!).

Create an image recognition model using data you curate, and deploy it on the web.
> yes, will do

What is DataLoaders?
> FastAI's DataLoader is a specialised tool that helps machine learning models efficiently process large amounts of data. Think of it like a smart conveyor belt in a factory - instead of trying to process all the data at once, it breaks it down into manageable chunks (called batches) and feeds them into the model one at a time. This not only makes the learning process more efficient but also helps the computer manage its memory better. The DataLoader can do several helpful things automatically: it can shuffle the data (like mixing up flashcards before studying), process multiple chunks at the same time (like having several conveyor belts working in parallel), and customize how the data is prepared before being fed to the model. While it's built on top of a similar tool from PyTorch (another popular machine learning framework), FastAI's version adds extra features to make it more flexible and easier to use.

What four things do we need to tell fastai to create DataLoaders?
> To turn our downloaded data into a DataLoaders object we need to tell fastai at least four things:
>
> - What kinds of data we are working with
> - How to get the list of items
> - How to label these items
> - How to create the validation set

What does the splitter parameter to DataBlock do?
> to split our training and validation sets randomly.

How do we ensure a random split always gives the same validation set?
> we manually type in a number and leave that same number as the seed to ensure the same results (the same images are split from the source list into test and validation sets)

What letters are often used to signify the independent and dependent variables?
> The independent variable is often referred to as x and the dependent variable is often referred to as y. 

What's the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?
> Crop truncates part of input images so the outputs are the same size. Pad adds black bands on images to make them the same size as other images. Squish distorts images, squishing some images down to match the shape of others in the set. You would be better off, so as not to distort images too much or have useless black padding that needlessly consumes compute.

What is data augmentation? Why is it needed?
> Data augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes. 
> It is needed 

What is the difference between item_tfms and batch_tfms?
> Item transforms are applied to individual images (like resising), while batch transforms are applied to groups of images simultaneously (like augmentation) and can use GPU acceleration

What is a confusion matrix?
> A table that shows how many times a model made correct and incorrect predictions for each category, helping visualize where the model makes mistakes.

What does export save?
> Export saves the model architecture, trained parameters, and DataLoader settings all in one file for easy deployment.

What is it called when we use a model for getting predictions, instead of training?
> This is called inference.

What are IPython widgets?
> Interactive GUI components that can be used in Jupyter notebooks to create user interfaces.

When might you want to use CPU for deployment? When might GPU be better?
> Use CPU when serving one prediction at a time to individual users (cheaper, simpler). Use GPU when processing many predictions simultaneously or need very fast processing.

What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?
> - Requires internet connection
> - Added latency from network
> - Privacy concerns with sending data to server
> - Costs for server maintenance

What are three examples of problems that could occur when rolling out a bear warning system in practice?
> - Poor performance in low light/nighttime
> - Difficulty with partially hidden bears
> - Camera quality and resolution issues
> - Bears in unusual positions not seen in training

What is "out-of-domain data"?
> Data that's different from what the model was trained on (like nighttime images when trained only on daytime ones).

What is "domain shift"?
> When the type of data the model sees gradually changes over time from what it was trained on.

What are the three steps in the deployment process?
> 1. Run model in parallel with manual process
> 2. Limited trial with human supervision
> 3. Gradual expansion with monitoring

Consider how the Drivetrain Approach maps to a project or problem you're interested in.
> - The objective is to automatically track and quantify individual athletes' movements during group workouts where everyone performs the same structured workout (like 5x5 deadlifts) within defined time windows, but at their own pace within those windows. 
> - The key levers we control are the wearable's sensors, their placement, and our knowledge of both the workout structure and time windows (e.g., that everyone is doing deadlifts somewhere within each 2-minute window).
> - We need time-series motion data from athletes performing these movements within the structured time blocks, including data on how different athletes naturally pace their sets and rest periods while still following the overall workout timing.
> - The models will need to handle movement classification and rep counting for each athlete individually, while using the known workout schedule and time windows as a context layer to improve accuracy, understanding that while everyone is doing the same movement pattern, they won't be perfectly synchronised.

When might it be best to avoid certain types of data augmentation?
> Avoid data augmentation when it creates unrealistic scenarios that would never occur in real usage - like flipping medical images or distorting text that needs to be read.

For a project you're interested in applying deep learning to, consider the thought experiment "What would happen if it went really, really well?"
> â€¢ The wearable would completely transform how we interact with technology by mastering nuanced movement recognition - starting with complex gym movements like clean and jerks, and evolving into a new form of precise human-computer interface

> â€¢ Success in the initial strength training market would prove that wearables can understand and respond to sophisticated human movements without any manual input, shifting us from "telling" devices what we're doing to "showing" them

> â€¢ The technology could scale from dominating the niche fitness market to revolutionizing how the general population interacts with computers and devices, using movement as a natural, frictionless input method

Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you're interested in.
> ok will do...
