{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using Pexels API for Image Collection with fastai\n",
        "\n",
        "This notebook demonstrates how to collect and organize image datasets using the Pexels API and fastai's utilities.\n",
        "\n",
        "## Key Concepts:\n",
        "1. **Image Collection**: We use Pexels as an alternative to Bing Image Search to gather images programmatically.\n",
        "   \n",
        "2. **fastai's download_images**: This is a powerful utility function that:\n",
        "   - Takes a list of URLs\n",
        "   - Downloads them efficiently\n",
        "   - Handles errors gracefully\n",
        "   - Saves images directly to specified folders\n",
        "\n",
        "3. **Directory Organization**: We create a structured dataset by:\n",
        "   - Making a main directory for our project\n",
        "   - Creating subdirectories for each category\n",
        "   - Saving images in their respective categories\n",
        "\n",
        "4. **Data Verification**: We use fastai's verify_images to:\n",
        "   - Check for corrupted downloads\n",
        "   - Remove problematic files\n",
        "   - Ensure our dataset is clean and ready for model training\n",
        "\n",
        "## Process Flow:\n",
        "1. Get image URLs from Pexels API\n",
        "2. Create organized folder structure\n",
        "3. Download images to appropriate folders\n",
        "4. Verify and clean the dataset\n",
        "5. Generate a summary report\n",
        "\n",
        "This approach sets us up for the next steps in the deep learning process by creating a well-organized image dataset that's ready for training.\n",
        "\n",
        "Note: While the original course used Bing's API, this notebook shows how to adapt the same principles using Pexels, demonstrating how fastai's utilities work independently of the image source."
      ],
      "metadata": {
        "id": "p-qg926fUjFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 - Setup and Imports"
      ],
      "metadata": {
        "id": "pXZ4m_aQSIAK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-1hJ7mMJR9tz"
      },
      "outputs": [],
      "source": [
        "# Setup for Google Colab\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import requests\n",
        "from fastbook import *\n",
        "from fastai.vision.widgets import *\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 - Function Definition and API Key"
      ],
      "metadata": {
        "id": "VEO632yVSMcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_images_pexels(api_key, query, per_page=80):\n",
        "    \"\"\"Search for images on Pexels and return an L object with contentUrl attribute.\"\"\"\n",
        "    headers = {'Authorization': api_key}\n",
        "    url = \"https://api.pexels.com/v1/search\"\n",
        "\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': per_page\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Error fetching data: {response.status_code}\")\n",
        "\n",
        "    data = response.json()\n",
        "    # Create an L object to match Bing's structure\n",
        "    urls = L([{'contentUrl': photo['src']['original']} for photo in data['photos']])\n",
        "    return urls\n",
        "\n",
        "# Set your Pexels API key\n",
        "key = os.environ.get('PEXELS_API_KEY', 'MgHvaLKOOwPoAfmIqoC1uIhrhW0T1IsFypI2WuTOrc2SlOdf4bvS5zVv')  # Replace XXX with your key"
      ],
      "metadata": {
        "id": "eXEVKWhTSO9t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 - Download Images"
      ],
      "metadata": {
        "id": "Wm5UPfLASTlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your categories\n",
        "bear_types = 'grizzly','black','teddy'\n",
        "\n",
        "# Create main directory\n",
        "path = Path('bears')\n",
        "if not path.exists():\n",
        "    path.mkdir()\n",
        "    for o in bear_types:\n",
        "        dest = (path/o)\n",
        "        dest.mkdir(exist_ok=True)\n",
        "        results = search_images_pexels(key, f'{o} bear')\n",
        "        download_images(dest, urls=results.attrgot('contentUrl'))\n",
        "\n",
        "# Verify the files were downloaded\n",
        "fns = get_image_files(path)\n",
        "fns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bIikwuwSWDV",
        "outputId": "9fd3ad16-46b8-4b78-cf37-e0c784ea107b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#240) [Path('bears/black/0fa056c6-96be-4bb1-adb9-a98aabf36f24.jpeg'),Path('bears/black/6c11dd31-1939-4efd-b1c2-ff40072bada6.jpeg'),Path('bears/black/120912bd-d1b0-4e14-a950-80f95ec3fd56.jpeg'),Path('bears/black/3c1dbae9-9e32-4d15-95db-1d24f083e9d9.jpeg'),Path('bears/black/7865bf20-1fc7-4468-bcc8-68a1f8e58b7e.jpeg'),Path('bears/black/7f8bf0fb-8b2d-4bda-8f1f-6cf340d12b6f.jpeg'),Path('bears/black/df526cec-e6ce-4690-ace7-cc47ee51848e.jpeg'),Path('bears/black/ebb98d9d-d6f2-4d07-94c3-61c21f850976.jpeg'),Path('bears/black/285270e3-4b60-4a74-9cb9-ed820349a6f2.jpeg'),Path('bears/black/485ecf23-4407-4b99-aef5-f456884ae085.jpeg'),Path('bears/black/6bfb47f7-f9f9-4763-b1e0-438c0f0a68c2.jpeg'),Path('bears/black/af9689a9-914d-4d94-b42c-2e83fc0ef409.jpeg'),Path('bears/black/a0428795-c54d-4a60-94d3-351a2408027e.jpeg'),Path('bears/black/e8ff71f7-0704-42fc-a64c-aa211ea4c862.jpeg'),Path('bears/black/e8d4aef6-d251-4efb-aaa1-2d4e86686635.jpeg'),Path('bears/black/b472a0cc-797a-4359-9278-485bf20aa8e5.jpeg'),Path('bears/black/0451a662-1714-4653-ad02-58da2b7cd51f.jpeg'),Path('bears/black/e822c9dc-1b2b-4500-ab18-c1ceac921318.jpeg'),Path('bears/black/115c4e1a-7885-4b35-a327-863947118cdc.jpeg'),Path('bears/black/5a35ec90-4ab2-4f3e-9dfc-2001bc6725c0.jpeg')...]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4 - Verify and Clean Images"
      ],
      "metadata": {
        "id": "82EwlHSCSYI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for corrupted images\n",
        "failed = verify_images(fns)\n",
        "failed\n",
        "\n",
        "# Remove corrupted images\n",
        "failed.map(Path.unlink);"
      ],
      "metadata": {
        "id": "ySplhgarSZRW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5 - Summary Report"
      ],
      "metadata": {
        "id": "sko3BrHsTlux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get counts for each category\n",
        "print(\"=== Download Summary ===\")\n",
        "for bear_type in bear_types:\n",
        "    category_path = path/bear_type\n",
        "    category_files = get_image_files(category_path)\n",
        "    print(f\"\\n{bear_type.title()} Bears:\")\n",
        "    print(f\"- Successfully downloaded: {len(category_files)} images\")\n",
        "    print(f\"- Saved in: {category_path}\")\n",
        "\n",
        "print(\"\\n=== Error Report ===\")\n",
        "if len(failed) > 0:\n",
        "    print(f\"Found and removed {len(failed)} corrupted images:\")\n",
        "    for f in failed:\n",
        "        print(f\"- {f.name}\")\n",
        "else:\n",
        "    print(\"No corrupted images were found!\")\n",
        "\n",
        "print(\"\\n=== Final Results ===\")\n",
        "final_files = get_image_files(path)\n",
        "print(f\"Total images across all categories: {len(final_files)}\")\n",
        "print(f\"Average images per category: {len(final_files)/len(bear_types):.1f}\")\n",
        "print(f\"\\nImages are ready for use in: {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cg4s_uSTo6s",
        "outputId": "7ee37ad6-a095-43c0-d8ab-7dce0dbda3a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Download Summary ===\n",
            "\n",
            "Grizzly Bears:\n",
            "- Successfully downloaded: 80 images\n",
            "- Saved in: bears/grizzly\n",
            "\n",
            "Black Bears:\n",
            "- Successfully downloaded: 80 images\n",
            "- Saved in: bears/black\n",
            "\n",
            "Teddy Bears:\n",
            "- Successfully downloaded: 80 images\n",
            "- Saved in: bears/teddy\n",
            "\n",
            "=== Error Report ===\n",
            "No corrupted images were found!\n",
            "\n",
            "=== Final Results ===\n",
            "Total images across all categories: 240\n",
            "Average images per category: 80.0\n",
            "\n",
            "Images are ready for use in: bears\n"
          ]
        }
      ]
    }
  ]
}
